{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Final Project (Group 14)\"\n",
        "format: html\n",
        "---\n",
        "\n",
        "\n",
        "# Import Packages"
      ],
      "id": "69df67da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import altair as alt\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.cluster import KMeans\n",
        "from textblob import TextBlob\n",
        "import spacy"
      ],
      "id": "730e1c82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Illegal Pets Data and Education Attainment Data in NYC"
      ],
      "id": "62b28d74"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Illegal Pets Data\n",
        "illegal_pets = pd.read_csv(\"data/Illegal Pets/illegal_animals_kept_as_pets_20241116.csv\")\n",
        "\n",
        "# Education Attainment Data\n",
        "education = pd.read_csv(\"data/Education Attainment/ACSST5Y2010.S1501-Data.csv\")"
      ],
      "id": "299a77d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combine all education data from 2010 to 2022"
      ],
      "id": "e73a57dc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Directory where all CSV files are stored\n",
        "directory = \"data/Education Attainment\"\n",
        "\n",
        "# Collect all file paths in the directory that match the naming pattern\n",
        "csv_files = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(\".csv\")]\n",
        "\n",
        "# List to hold individual DataFrames\n",
        "dataframes = []\n",
        "\n",
        "# Loop through each file, read it into a DataFrame, and append it to the list\n",
        "for file in csv_files:\n",
        "    # Extract the year from the file name (e.g., \"ACSST5Y2010\" -> \"2010\")\n",
        "    year = os.path.basename(file).split(\"ACSST5Y\")[1][:4]\n",
        "    df = pd.read_csv(file)\n",
        "    df['YEAR'] = year\n",
        "    \n",
        "    # Append the modified DataFrame to the list\n",
        "    dataframes.append(df)\n",
        "\n",
        "# Concatenate all DataFrames into one\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Sort the combined DataFrame by 'GEO_ID' in descending order\n",
        "sorted_df = combined_df.sort_values(by='GEO_ID', ascending=False)"
      ],
      "id": "ec09bea0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anayze meanings of variables\n",
        "Each year's dataset's variable has different interpretation, so it it critical to firstly select the most relevant variable for futher analysis. In our project, we only select total estimate to simplify our anlysis without considering gender and earning. For example, S1501_C01_015's definition has changed in 2018 from 'percent of bachelor degree or higher' to 'total population of bachelor degreee or higher.' Thus, we have to manually clean these messy variables."
      ],
      "id": "56e8b19a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select the top 13 rows (containing each years variable definition) to analyze vairbales meaning\n",
        "top_13_rows = sorted_df.head(13)\n",
        "\n",
        "# Identify columns where any cell contains \"female\", \"male\", or \"margin\"\n",
        "columns_to_drop = top_13_rows.columns[\n",
        "    top_13_rows.apply(lambda col: col.astype(str).str.contains(r\"female|male|margin|nan|earning|race\", case=False, na=False).any())\n",
        "]\n",
        "\n",
        "# Drop the identified columns\n",
        "top_13_rows_f = top_13_rows.drop(columns=columns_to_drop)\n",
        "top_13_rows_f = top_13_rows_f.dropna(axis=1, how='all')"
      ],
      "id": "aac30f39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Select necessery variables"
      ],
      "id": "02e69565"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cleaning Education Attainment Data\n",
        "# Rename selected columns for better readability\n",
        "column_rename_map = {\n",
        "    \"GEO_ID\": \"geo_id\",  # Geographic identifier\n",
        "    \"NAME\": \"area_name\",  # Geographic area name\n",
        "    \"YEAR\": \"year\",  # Year of data\n",
        "    \"S1501_C01_006E\": \"pop_25_plus\",  # Population 25 years and over\n",
        "    \"S1501_C01_007E\": \"pop_25_less_9th\",  # 25 years and over: Less than 9th grade\n",
        "    \"S1501_C01_009E\": \"pop_25_hs_grad\",  # 25 years and over: High school graduate\n",
        "    \"S1501_C01_013E\": \"pop_25_bach_plus\",  # 25 years and over: Bachelor's degree or higher\n",
        "    \"S1501_C01_019E\": \"pop_25_34_bach_plus\",  # 25-34: Bachelor's degree or higher\n",
        "    \"S1501_C01_021E\": \"pop_45_64_bach_plus\"  # 45-64: Bachelor's degree or higher\n",
        "}\n",
        "\n",
        "# Create selected_columns based on column_rename_map keys\n",
        "selected_columns = list(column_rename_map.keys())\n",
        "\n",
        "# Filter the combined DataFrame to only keep the selected columns\n",
        "filtered_df = combined_df[selected_columns]\n",
        "\n",
        "# Rename the columns\n",
        "filtered_df.rename(columns=column_rename_map, inplace=True)\n",
        "\n",
        "# Remove rows where GEO_ID equals \"Geographic Area Name\"\n",
        "edu_cleaned = filtered_df[filtered_df[\"geo_id\"] != \"Geographic Area Name\"]\n",
        "\n",
        "# Adjust variables to accommodate different years' approaches to estimate (volume or percentage)\n",
        "# Transform year 2010 to year 2014's 'pop_25_less_9th' variable to be the result between 'pop_25_less_9th' and 'pop_25_plus'\n",
        "for year in range(2010, 2015):\n",
        "    edu_cleaned.loc[edu_cleaned['year'] == str(year), 'pop_25_less_9th'] = pd.to_numeric(edu_cleaned['pop_25_less_9th'], errors='coerce') * pd.to_numeric(edu_cleaned['pop_25_plus'], errors='coerce') / 100\n",
        "    edu_cleaned.loc[edu_cleaned['year'] == str(year), 'pop_25_hs_grad'] = pd.to_numeric(edu_cleaned['pop_25_hs_grad'], errors='coerce') * pd.to_numeric(edu_cleaned['pop_25_plus'], errors='coerce') / 100\n",
        "    edu_cleaned.loc[edu_cleaned['year'] == str(year), 'pop_25_bach_plus'] = pd.to_numeric(edu_cleaned['pop_25_bach_plus'], errors='coerce') * pd.to_numeric(edu_cleaned['pop_25_plus'], errors='coerce') / 100\n",
        "\n",
        "# Cleaning Illegal Pets Data\n",
        "# List of columns to drop\n",
        "columns_to_drop = [\n",
        "    'Agency', 'Agency Name', 'Complaint Type', 'Cross Street 1', 'Cross Street 2',\n",
        "    'Intersection Street 1', 'Intersection Street 2', 'City', 'Landmark',\n",
        "    'Facility Type', 'Community Board', 'Park Facility Name', 'Vehicle Type',\n",
        "    'Taxi Company Borough', 'Taxi Pick Up Location', 'Bridge Highway Name',\n",
        "    'Bridge Highway Direction', 'Road Ramp', 'Bridge Highway Segment'\n",
        "]\n",
        "\n",
        "# Drop the specified columns\n",
        "illegal_pets = illegal_pets.drop(columns=columns_to_drop, errors='ignore')"
      ],
      "id": "ba7a1559",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Standardization of County Name"
      ],
      "id": "fd296f94"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Exam unqiue values in both dataframes for furthern merging\n",
        "print(edu_cleaned['area_name'].unique())\n",
        "\n",
        "# Create county name list in NYC\n",
        "nyc_counties = [\n",
        "    \"New York County, New York\",  # Manhattan\n",
        "    \"Kings County, New York\",     # Brooklyn\n",
        "    \"Queens County, New York\",    # Queens\n",
        "    \"Bronx County, New York\",     # The Bronx\n",
        "    \"Richmond County, New York\"   # Staten Island\n",
        "]\n",
        "\n",
        "# Select counties in NYC and filter them\n",
        "edu_nyc = edu_cleaned[edu_cleaned['area_name'].isin(nyc_counties)]\n",
        "\n",
        "# Mapping of county names to simplified borough names\n",
        "county_replacement_map = {\n",
        "    \"New York County, New York\": \"New York\",\n",
        "    \"Kings County, New York\": \"Kings\",\n",
        "    \"Queens County, New York\": \"Queens\",\n",
        "    \"Bronx County, New York\": \"The Bronx\",\n",
        "    \"Richmond County, New York\": \"Staten Island\"\n",
        "}\n",
        "\n",
        "# Replace county names in the 'name' column using the map\n",
        "edu_nyc['area_name'] = edu_nyc['area_name'].replace(county_replacement_map)\n",
        "\n",
        "# Exam unqiue values in both dataframes for furthern merging\n",
        "print(illegal_pets['Borough'].unique())\n",
        "\n",
        "# Mapping Borough\n",
        "borough_to_county = {\n",
        "    \"BROOKLYN\": \"Kings\",\n",
        "    \"STATEN ISLAND\": \"Staten Island\",\n",
        "    \"QUEENS\": \"Queens\",\n",
        "    \"MANHATTAN\": \"New York\",\n",
        "    \"BRONX\": \"The Bronx\"\n",
        "}\n",
        "\n",
        "# Map the 'Borough' column to new 'County' values\n",
        "illegal_pets['area_name'] = illegal_pets['Borough'].map(borough_to_county)\n",
        "\n",
        "print(illegal_pets['area_name'].unique())\n",
        "print(edu_nyc['area_name'].unique())"
      ],
      "id": "d2eccbb9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Standardization Time Variable"
      ],
      "id": "7934fa51"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Format both columns as strings\n",
        "illegal_pets['year'] = pd.to_datetime(illegal_pets['Created Date'], errors='coerce').dt.strftime('%Y')\n",
        "edu_nyc['year'] = pd.to_datetime(edu_nyc['year'], errors='coerce').dt.strftime('%Y')"
      ],
      "id": "5272670b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Merging "
      ],
      "id": "f9a394e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Drop 2023 and 2024 because education data does not includes these years\n",
        "illegal_pets = illegal_pets[~illegal_pets['year'].isin(['2023', '2024'])]\n",
        "\n",
        "# Merge based on year and area_name (County name)\n",
        "merged_df = pd.merge(illegal_pets, edu_nyc, on=['year', 'area_name'], how='left')\n",
        "print(merged_df.head())\n",
        "\n",
        "# Save the merged DataFrame to a CSV file\n",
        "merged_df.to_csv(\"final_education_illegal_pets.csv\", index=False)"
      ],
      "id": "0e2b6721",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Map Data, drop NA and convert data type"
      ],
      "id": "3a9dddb1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the GeoJSON file\n",
        "geojson_path = 'nyc-zips.geojson'\n",
        "nyc_zips = gpd.read_file(geojson_path)\n",
        "\n",
        "# Drop rows with NaN ZIP codes or fill them with a placeholder\n",
        "merged_df = merged_df.dropna(subset=['Incident Zip'])\n",
        "\n",
        "# Convert ZIP codes to strings without decimals\n",
        "merged_df['Incident Zip'] = merged_df['Incident Zip'].apply(lambda x: str(int(x)))\n",
        "\n",
        "# Ensure both columns are strings\n",
        "nyc_zips['postalCode'] = nyc_zips['postalCode'].astype(str)"
      ],
      "id": "35b78c71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Final Project Visualization\n",
        "\n",
        "# 1. Bar chart: types of illegal pets in NYC"
      ],
      "id": "fe2c46c0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. Bar chart of types of illegal pets\n",
        "chart = alt.Chart(merged_df).mark_bar().encode(\n",
        "    y=alt.Y('Descriptor:N', \n",
        "            sort='-x',  # Sort by count in descending order\n",
        "            title='Pet Type'),\n",
        "    x=alt.X('count():Q',\n",
        "            title='Number of Incidents'),\n",
        "    tooltip=['Descriptor', alt.Tooltip('count():Q', title='Count')]\n",
        ").properties(\n",
        "    title='Types of Illegal Pets in NYC',\n",
        "    width=500,\n",
        "    height=400\n",
        ").configure_axis(\n",
        "    labelFontSize=12,\n",
        "    titleFontSize=14\n",
        ").configure_title(\n",
        "    fontSize=16,\n",
        "    anchor='middle'\n",
        ")\n",
        "\n",
        "chart\n",
        "chart.save(\"pictures/bar_chart_types_of_illegal_pets.png\")"
      ],
      "id": "5788159a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Pie chart of incidents across boroughs"
      ],
      "id": "f18b43d6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate percentages and create pie chart with labels\n",
        "total = len(merged_df)\n",
        "df_with_pct = merged_df.assign(\n",
        "    percentage=lambda x: (x.groupby('Borough')['Borough'].transform('count') / total * 100).round(1)\n",
        ")\n",
        "\n",
        "# Create a summary dataframe for the text labels\n",
        "summary_df = (df_with_pct.groupby('Borough')\n",
        "             .agg(count=('Borough', 'count'),\n",
        "                  percentage=('percentage', 'first'))\n",
        "             .reset_index()\n",
        "             .assign(label=lambda x: x['Borough'] + ': ' + x['percentage'].astype(str) + '%'))\n",
        "\n",
        "# Create pie chart with percentages\n",
        "chart = alt.Chart(df_with_pct).mark_arc(outerRadius=180).encode(\n",
        "    theta='count():Q',\n",
        "    color=alt.Color('Borough:N', \n",
        "                   scale=alt.Scale(scheme='tableau10'),\n",
        "                   legend=alt.Legend(title=\"Borough\")),\n",
        "    tooltip=['Borough:N', \n",
        "            alt.Tooltip('count():Q', title='Count'),\n",
        "            alt.Tooltip('percentage:Q', title='Percentage', format='.1f')]\n",
        ").properties(\n",
        "    title={\n",
        "        'text': 'Distribution of Illegal Pet Incidents by Borough',\n",
        "        'fontSize': 16,\n",
        "        'anchor': 'middle'\n",
        "    },\n",
        "    width=500,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "# Add text labels with percentages\n",
        "text = alt.Chart(summary_df).mark_text(radius=120, size=11).encode(\n",
        "    theta=alt.Theta('count:Q', stack=True),\n",
        "    text='label:N'\n",
        ")\n",
        "\n",
        "# Combine chart and labels\n",
        "final_chart = (chart + text).configure_view(\n",
        "    strokeWidth=0\n",
        ")\n",
        "\n",
        "final_chart\n",
        "final_chart.save(\"pictures/pie_chart_incidents_by_borough.png\")"
      ],
      "id": "68603959",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Choropleth Map"
      ],
      "id": "240e33e5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load GeoJSON file\n",
        "geojson_path = 'nyc-zips.geojson'\n",
        "nyc_zips = gpd.read_file(geojson_path)\n",
        "merged_df = pd.read_csv(\"final_education_illegal_pets.csv\")\n",
        "\n",
        "# Data Preprocessing\n",
        "merged_df = merged_df.dropna(subset=['Incident Zip'])\n",
        "merged_df['Incident Zip'] = merged_df['Incident Zip'].apply(lambda x: str(int(x)))\n",
        "nyc_zips['postalCode'] = nyc_zips['postalCode'].astype(str)\n",
        "\n",
        "# Count complaints by ZIP code\n",
        "pet_counts = merged_df.groupby('Incident Zip').size().reset_index(name='complaints_count')\n",
        "\n",
        "# Merge counts with GeoDataFrame\n",
        "nyc_zips = nyc_zips.merge(pet_counts, left_on='postalCode', right_on='Incident Zip', how='left')\n",
        "nyc_zips['complaints_count'] = nyc_zips['complaints_count'].fillna(0)\n",
        "\n",
        "# Convert GeoJSON to TopoJSON (Altair supports TopoJSON)\n",
        "geojson_data = json.loads(nyc_zips.to_json())\n",
        "\n",
        "# Create Altair Chart\n",
        "choropleth = alt.Chart(alt.Data(values=geojson_data['features'])).mark_geoshape().encode(\n",
        "    color=alt.Color('properties.complaints_count:Q', \n",
        "                    scale=alt.Scale(scheme='orangered'), \n",
        "                    title='Illegal Pet Complaints'),\n",
        "    tooltip=[\n",
        "        alt.Tooltip('properties.postalCode:N', title='ZIP Code'),\n",
        "        alt.Tooltip('properties.complaints_count:Q', title='Complaints')\n",
        "    ]\n",
        ").transform_calculate(\n",
        "    complaints_count='datum.properties.complaints_count'\n",
        ").properties(\n",
        "    title='Choropleth Map of Illegal Pet Complaints in NYC by ZIP Code',\n",
        "    width=500,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "# Display the chart\n",
        "choropleth\n",
        "choropleth.save(\"pictures/choropleth_map_illegal_pets.png\")"
      ],
      "id": "1c3b622b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Line Plot: Different education level over time"
      ],
      "id": "9c5c4d26"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import altair as alt\n",
        "\n",
        "# Prepare the data (keeping the same data preparation steps)\n",
        "education_levels_over_time = merged_df.groupby('year')[\n",
        "    ['pop_25_less_9th', 'pop_25_hs_grad', 'pop_25_bach_plus']\n",
        "].mean().reset_index()\n",
        "\n",
        "education_levels_long = education_levels_over_time.melt(\n",
        "    id_vars='year',\n",
        "    value_vars=['pop_25_less_9th', 'pop_25_hs_grad', 'pop_25_bach_plus'],\n",
        "    var_name='Education Level',\n",
        "    value_name='Population'\n",
        ")\n",
        "\n",
        "education_level_mapping = {\n",
        "    'pop_25_less_9th': 'Less than 9th Grade',\n",
        "    'pop_25_hs_grad': 'High School Graduate',\n",
        "    'pop_25_bach_plus': 'Bachelor\\'s Degree or Higher'\n",
        "}\n",
        "education_levels_long['Education Level'] = education_levels_long['Education Level'].map(education_level_mapping)\n",
        "\n",
        "# Create the line chart using Altair\n",
        "chart = alt.Chart(education_levels_long).mark_line(point=True).encode(\n",
        "    x=alt.X('year:O', title='Year'),\n",
        "    y=alt.Y('Population:Q', title='Average Population'),\n",
        "    color=alt.Color('Education Level:N', title='Education Level'),\n",
        "    tooltip=['year', 'Education Level', alt.Tooltip('Population:Q', format=',')]\n",
        ").properties(\n",
        "    title='Different Education Levels Over Time',\n",
        "    width=500,\n",
        "    height=400\n",
        ").configure_axis(\n",
        "    labelFontSize=12,\n",
        "    titleFontSize=14\n",
        ").configure_title(\n",
        "    fontSize=16\n",
        ")\n",
        "\n",
        "chart"
      ],
      "id": "ecea691b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Bar Plot: Different education level by districts"
      ],
      "id": "66821233"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import altair as alt\n",
        "import pandas as pd\n",
        "\n",
        "# Aggregate education data by borough\n",
        "education_levels_by_district = merged_df.groupby('Borough')[\n",
        "    ['pop_25_less_9th', 'pop_25_hs_grad', 'pop_25_bach_plus']\n",
        "].mean().reset_index()\n",
        "\n",
        "# Melt the data for easier plotting with Altair\n",
        "education_levels_long = education_levels_by_district.melt(\n",
        "    id_vars='Borough',\n",
        "    value_vars=['pop_25_less_9th', 'pop_25_hs_grad', 'pop_25_bach_plus'],\n",
        "    var_name='Education Level',\n",
        "    value_name='Population'\n",
        ")\n",
        "\n",
        "# Map the correct labels to the \"Education Level\" column\n",
        "education_level_mapping = {\n",
        "    'pop_25_less_9th': 'Less than 9th Grade',\n",
        "    'pop_25_hs_grad': 'High School Graduate',\n",
        "    'pop_25_bach_plus': 'Bachelor\\'s Degree or Higher'\n",
        "}\n",
        "education_levels_long['Education Level'] = education_levels_long['Education Level'].map(education_level_mapping)\n",
        "\n",
        " \n",
        "# Create Altair chart\n",
        "chart = alt.Chart(education_levels_long).mark_bar().encode(\n",
        "    x=alt.X('Borough:N', title='Borough'),\n",
        "    y=alt.Y('Population:Q', title='Average Population'),\n",
        "    color=alt.Color('Education Level:N', title='Education Level'),\n",
        "    tooltip=['Borough', 'Education Level', 'Population']\n",
        ").properties(\n",
        "    title='Different Education Levels by District',\n",
        "    width=500,\n",
        "    height=400\n",
        ").configure_axis(\n",
        "    labelFontSize=12,\n",
        "    titleFontSize=14\n",
        ").configure_title(\n",
        "    fontSize=16,\n",
        "    anchor='middle'\n",
        ")\n",
        "\n",
        "chart"
      ],
      "id": "de177f00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Correlation Analysis\n"
      ],
      "id": "d4b27a2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import folium\n",
        "\n",
        "# 2. Aggregate Data\n",
        "# Count illegal pet incidents by district\n",
        "incident_counts = merged_df.groupby('Borough')['Descriptor'].count().reset_index()\n",
        "incident_counts.rename(columns={'Descriptor': 'Illegal_Pet_Incidents'}, inplace=True)\n",
        "\n",
        "# Calculate mean education levels by district\n",
        "education_data = merged_df.groupby('Borough')[\n",
        "    ['pop_25_less_9th', 'pop_25_hs_grad', 'pop_25_bach_plus', 'pop_25_plus']\n",
        "].mean().reset_index()\n",
        "\n",
        "# Merge datasets\n",
        "district_data = pd.merge(incident_counts, education_data, on='Borough')\n",
        "\n",
        "# Calculate the percentage of each education level relative to the total population\n",
        "district_data['pop_25_less_9th_percentage'] = (district_data['pop_25_less_9th'] / district_data['pop_25_plus']) * 100\n",
        "district_data['pop_25_hs_grad_percentage'] = (district_data['pop_25_hs_grad'] / district_data['pop_25_plus']) * 100\n",
        "district_data['pop_25_bach_plus_percentage'] = (district_data['pop_25_bach_plus'] / district_data['pop_25_plus']) * 100\n",
        "\n",
        "# Calculate incident percentage\n",
        "district_data['Incident_Percentage(%)'] = (district_data['Illegal_Pet_Incidents'] / district_data['pop_25_plus']) * 100\n",
        "\n",
        "# 3. Perform Regression: Using all education levels (as percentages) as predictors\n",
        "X = district_data[['pop_25_less_9th_percentage', 'pop_25_hs_grad_percentage', 'pop_25_bach_plus_percentage']]\n",
        "y = district_data['Incident_Percentage(%)']\n",
        "\n",
        "# Add a constant to the model (for the intercept)\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "print(model.summary())\n",
        "\n",
        "import altair as alt\n",
        "import pandas as pd\n",
        "\n",
        "# Use Altair to create regression plots for each feature\n",
        "for feature, label in zip(\n",
        "    ['pop_25_less_9th_percentage', 'pop_25_hs_grad_percentage', 'pop_25_bach_plus_percentage'],\n",
        "    [\"% Population with Less than 9th Grade Education\",\n",
        "     \"% Population with High School Graduation\",\n",
        "     \"% Population with Bachelor's Degree or Higher\"]\n",
        "):\n",
        "    # Determine the min and max for the x-axis range with a little padding\n",
        "    x_min = district_data[feature].min() * 0.9  # 10% padding below min\n",
        "    x_max = district_data[feature].max() * 1.1  # 10% padding above max\n",
        "\n",
        "    # Create Altair chart\n",
        "    chart = alt.Chart(district_data).mark_point(filled=True, size=100).encode(\n",
        "        x=alt.X(\n",
        "            f'{feature}:Q', \n",
        "            title=label, \n",
        "            scale=alt.Scale(domain=[x_min, x_max])  # Dynamically set x-axis range\n",
        "        ),\n",
        "        y=alt.Y('Incident_Percentage(%):Q', title=\"Illegal Pet Incident Percentage (%)\"),\n",
        "        tooltip=[\n",
        "            'Borough', \n",
        "            alt.Tooltip(f'{feature}:Q', title=label), \n",
        "            alt.Tooltip('Incident_Percentage(%):Q', title='Incident %')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Add district names as text annotations\n",
        "    text_labels = alt.Chart(district_data).mark_text(\n",
        "        align='left',  \n",
        "        dx=7,   \n",
        "        fontSize=8   \n",
        "    ).encode(\n",
        "        x=alt.X(f'{feature}:Q'),\n",
        "        y=alt.Y('Incident_Percentage(%):Q'),\n",
        "        text='Borough'  \n",
        "    )\n",
        "\n",
        "    # Add regression line\n",
        "    regression_line = chart.transform_regression(\n",
        "        f'{feature}', 'Incident_Percentage(%)'\n",
        "    ).mark_line(color='red')\n",
        "\n",
        "    # Combine scatter plot and regression line\n",
        "    combined_chart = chart + regression_line + text_labels\n",
        "\n",
        "    # Display the chart directly\n",
        "    display(combined_chart.properties(\n",
        "        title=f\"Regression: Illegal Pet Incident Percentage vs. {label}\",\n",
        "        width=800,\n",
        "        height=400\n",
        "    ).configure_title(\n",
        "        fontSize=10,\n",
        "        anchor='middle'\n",
        "    ).configure_axis(\n",
        "        labelFontSize=8,\n",
        "        titleFontSize=10\n",
        "    ))\n",
        "    combined_chart.save(f\"pictures/regression_plot_{feature}.png\")"
      ],
      "id": "71110d91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Why Use This Method\n",
        "This method normalizes incident counts by population size, allowing fair comparisons across boroughs. Regression analysis helps identify relationships between education levels as percentages and incident rates, while visualization makes these patterns clear.\n",
        "\n",
        "By normalizing incident counts per 10,000 people, the method ensures fair comparisons across boroughs with different population sizes. Regression analysis quantifies the relationship between education levels and incident rates, providing insights into potential trends. Visualizations, such as scatter plots, make these relationships clear and accessible, helping to identify patterns and inform decision-making.\n",
        "\n",
        "# Analysis of the Graphs\n",
        "The regression coefficients are very small, reflecting the rare nature of illegal pet ownership (0.2% to 1.0% of population). Less than 9th-grade education shows a negative coefficient (-0.0466), high school graduation shows a slight negative relationship (-0.0335), and bachelor's degree shows a negative relationship (-0.0400).\n",
        "\n",
        "The graphs appear to show strong negative relationships primarily due to scaling effects. While the x-axis spans a large range (showing education levels in thousands), the y-axis variation is minimal (0.2% to 1.0% incident rate). This disparity in scales makes even small changes appear more dramatic visually. For instance, the positive coefficient for high school graduation (3.576e-06) appears negative in the graph because the effect is so small relative to the axis scales. These patterns, while statistically subtle, suggest that education levels have a minor influence on illegal pet ownership in NYC, though other factors likely play important roles in these relationships.\n",
        "\n",
        "# Limitations\n",
        "The analysis is limited by the small sample size, with only five boroughs, which reduces the statistical power and makes it difficult to draw definitive conclusions. The use of borough-level aggregation may mask important variations within boroughs, potentially overlooking localized trends.  Additionally, while the analysis identifies correlations between education levels and incident rates, it cannot establish causation. \n",
        "\n",
        "# Conclusion\n",
        "Despite its limitations, this method offers a useful framework for understanding the relationship between educational attainment and illegal pet incidents in NYC. By normalizing data and employing regression analysis, the study provides insights that can inform policy decisions and highlight areas for further research. The visualizations effectively communicate complex data, making it accessible to a wider audience and supporting informed decision-making.\n"
      ],
      "id": "b27c2c90"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. Download NYC borough boundaries GeoJSON\n",
        "url = \"https://data.cityofnewyork.us/api/geospatial/7t3b-ywvw?method=export&format=GeoJSON\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Save the GeoJSON to a file\n",
        "geojson_path = \"nyc_boroughs.geojson\"\n",
        "with open(geojson_path, \"wb\") as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "# Load the GeoJSON data\n",
        "boroughs_gdf = gpd.read_file(geojson_path)\n",
        "# Merge regression data with GeoJSON\n",
        "map_data = boroughs_gdf.merge(district_data, left_on='boro_name', right_on='Borough', how='left')\n",
        "\n",
        "# Create the map\n",
        "m = folium.Map(location=[40.7128, -74.0060], zoom_start=10)\n",
        "\n",
        "# Add choropleth for regression residuals\n",
        "map_data['Residuals'] = model.resid  \n",
        "folium.Choropleth(\n",
        "    geo_data=map_data,\n",
        "    data=map_data,\n",
        "    columns=['boro_name', 'Residuals'],\n",
        "    key_on='feature.properties.boro_name',\n",
        "    fill_color='OrRd',\n",
        "    fill_opacity=0.7,\n",
        "    line_opacity=0.2,\n",
        "    legend_name=\"Regression Residuals\",\n",
        ").add_to(m)\n",
        "\n",
        "# Add district names as labels\n",
        "for _, row in map_data.iterrows():\n",
        "    folium.Marker(\n",
        "        location=[row.geometry.centroid.y, row.geometry.centroid.x],  # Use centroid for label position\n",
        "        popup=f\"Borough: {row['boro_name']}\\nResidual: {row['Residuals']:.2f}\",\n",
        "        icon=folium.DivIcon(html=f\"<div style='font-size: 12px; color: black;'>{row['boro_name']}</div>\")\n",
        "    ).add_to(m)\n",
        "\n",
        "# Save and display the map\n",
        "m.save(\"regression_map_with_labels.html\")\n",
        "print(\"Map saved as 'regression_map_with_labels.html'. Open this file to view it.\")"
      ],
      "id": "ab480e34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The choropleth map of regression residuals reveals varying model performance across NYC boroughs, with the Bronx showing higher actual incident rates than predicted (red), Manhattan aligning closely with predictions (light orange), and Queens/Brooklyn showing moderate deviations (orange). These patterns suggest that while our education-based regression model captures some patterns in illegal pet ownership, it may not account for other important factors affecting incident rates, particularly in the Bronx. Limitations of this visualization include potential oversimplification of spatial patterns due to borough-level aggregation, which masks within-borough variations, and the challenge of interpreting residuals without considering the underlying population density or socioeconomic factors beyond education that might influence illegal pet ownership patterns.\n"
      ],
      "id": "224afd2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "def analyze_resolutions_nlp():\n",
        "    # Simple preprocessing without NLTK\n",
        "    def preprocess_text(text):\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        return str(text).lower()\n",
        "    \n",
        "    # Add sentiment analysis\n",
        "    def get_sentiment(text):\n",
        "        if pd.isna(text):\n",
        "            return 0\n",
        "        blob = TextBlob(str(text))\n",
        "        return blob.sentiment.polarity\n",
        "    \n",
        "    # Create preprocessed text column and sentiment scores\n",
        "    merged_df['processed_resolution'] = merged_df['Resolution Description'].apply(preprocess_text)\n",
        "    merged_df['sentiment_score'] = merged_df['Resolution Description'].apply(get_sentiment)\n",
        "    \n",
        "    # Rest of your existing category templates and TF-IDF code...\n",
        "    category_templates = {\n",
        "        'Warning Issued': 'warning documentation owner landlord violation instruct correct',\n",
        "        'Access Denied': 'could not access unable to gain access site inspect property',\n",
        "        'No Violations Found': 'no violations found no violations cited inspection conducted',\n",
        "        'Violations Found': 'notice of violation violations rules regulations issued',\n",
        "        'Referred Other Agency': 'not within jurisdiction referred to another agency',\n",
        "        'Under Investigation': 'will review in process investigating inspection',\n",
        "        'Invalid Information': 'could not locate incorrect address incomplete information',\n",
        "        'Case Closed': 'reviewed and closed request complete'\n",
        "    }\n",
        "    \n",
        "    vectorizer = TfidfVectorizer(lowercase=True, stop_words='english', max_features=1000)\n",
        "    template_vectors = vectorizer.fit_transform(category_templates.values())\n",
        "    description_vectors = vectorizer.transform(merged_df['processed_resolution'])\n",
        "    \n",
        "    def get_category(vector):\n",
        "        if vector.nnz == 0:\n",
        "            return \"No Resolution Recorded\"\n",
        "        similarities = cosine_similarity(vector, template_vectors)\n",
        "        best_match_idx = similarities.argmax()\n",
        "        if similarities[0][best_match_idx] < 0.1:\n",
        "            return \"Other Resolution\"\n",
        "        return list(category_templates.keys())[best_match_idx]\n",
        "    \n",
        "    # Apply categorization\n",
        "    merged_df['resolution_category'] = [get_category(vector) for vector in description_vectors]\n",
        "    \n",
        "    # Create category statistics\n",
        "    category_stats = merged_df['resolution_category'].value_counts()\n",
        "    \n",
        "    # Calculate average sentiment by category\n",
        "    sentiment_by_category = merged_df.groupby('resolution_category')['sentiment_score'].agg(['mean', 'count']).round(3)\n",
        "    \n",
        "    # Create visualization data\n",
        "    chart_data = pd.DataFrame({\n",
        "        'Resolution_Type': category_stats.index,\n",
        "        'Count': category_stats.values\n",
        "    })\n",
        "    \n",
        "    # Create category distribution chart\n",
        "    resolution_chart = alt.Chart(chart_data).mark_bar().encode(\n",
        "        x=alt.X('Count:Q', title='Number of Cases'),\n",
        "        y=alt.Y('Resolution_Type:N', sort='-x', title='Resolution Type'),\n",
        "        tooltip=['Resolution_Type', 'Count']\n",
        "    ).properties(\n",
        "        title='Distribution of Complaint Resolutions',\n",
        "        width=600,\n",
        "        height=400\n",
        "    )\n",
        "    \n",
        "    # Create sentiment by category chart\n",
        "    sentiment_data = merged_df.groupby('resolution_category')['sentiment_score'].mean().reset_index()\n",
        "    sentiment_chart = alt.Chart(sentiment_data).mark_bar().encode(\n",
        "        x=alt.X('sentiment_score:Q', title='Average Sentiment Score'),\n",
        "        y=alt.Y('resolution_category:N', sort='-x', title='Resolution Type'),\n",
        "        color=alt.Color('sentiment_score:Q', \n",
        "                       scale=alt.Scale(scheme='redblue'),\n",
        "                       title='Sentiment'),\n",
        "        tooltip=['resolution_category', \n",
        "                alt.Tooltip('sentiment_score:Q', format='.3f')]\n",
        "    ).properties(\n",
        "        title='Average Sentiment by Resolution Type',\n",
        "        width=600,\n",
        "        height=400\n",
        "    )\n",
        "    \n",
        "    # Print statistics\n",
        "    print(\"\\nResolution Statistics:\")\n",
        "    print(category_stats)\n",
        "    print(\"\\nPercentage Distribution:\")\n",
        "    print((category_stats / len(merged_df) * 100).round(2))\n",
        "    print(\"\\nSentiment Analysis by Category:\")\n",
        "    print(sentiment_by_category)\n",
        "    \n",
        "    # Return both charts side by side\n",
        "    combined_chart = alt.hconcat(resolution_chart, sentiment_chart)\n",
        "    return combined_chart, category_stats, sentiment_by_category\n",
        "\n",
        "# Run the analysis\n",
        "combined_chart, stats, sentiment_stats = analyze_resolutions_nlp()\n",
        "combined_chart"
      ],
      "id": "c09460bf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We applied sentiment analysis to understand the emotional tone in resolution descriptions of illegal pet complaints. Using TextBlob, we scored text from -1 (negative) to +1 (positive), with 0 being neutral, to understand how different outcomes are communicated.\n",
        "\n",
        "Key Findings:\n",
        "\n",
        "Most Common Resolution: \"Warning Issued\" (34% of cases)\n",
        "Most Negative Tone: \"Access Denied\" (-0.192)\n",
        "Most Neutral: \"Violations Found\" (-0.010)\n",
        "Overall Trend: Slightly negative to neutral tone across most categories\n",
        "\n",
        "The analysis reveals that while most communications maintain a professional, slightly negative tone, failed inspection attempts (\"Access Denied\") are communicated more negatively. This suggests a consistent official communication style with variations based on resolution outcome.\n",
        "\n",
        "Resolution Distribution Analysis:\n",
        "\n",
        "Major Categories:\n",
        "\n",
        "\"Warning Issued\" leads (1,335 cases, 34%)\n",
        "\"No Resolution Recorded\" second (825 cases, 21%)\n",
        "Three categories tie at ~500 cases each (12-14%): \"Case Closed,\" \"No Violations Found,\" \"Access Denied\"\n",
        "\n",
        "Key Insight:\n",
        "The data shows a preference for issuing warnings over finding violations, though a concerning 21% lack recorded resolutions. Access to inspection sites remains a notable challenge."
      ],
      "id": "a7b84810"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/envs/geo_env/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}